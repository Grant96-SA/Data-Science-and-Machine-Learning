{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display\n",
    "\n",
    "## Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have decided to take a reinforcement learning approach to the question due to the future reward aspect. My goal is to use Q learning to determine the optimal route from the top of the triangle. Q learning is an off policy reinforcement learning technique using temporal difference (TD).\n",
    "\n",
    "General terms:\n",
    "- A policy is the probability of taking an action given the state the agent is in i.e. a relationship between state and action\n",
    "- An action-value array (Q(state,action)) is an array of actions by state which is an approximation of the expected reward for taking a specific action in a specific state.\n",
    "- Q-learning is off-policy because regardless of the action taken, the action-value array (Q) is updated with respect to the maximum expected future reward for being in the next state. This allows our agent over time to determine the optimal policy regardless if an action didn't return the best possible reward\n",
    "- temporal difference (td) methods update the action value function using step wise updates instead of waiting for the entire reinforcement learning run to complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the path to your image\n",
    "image_path = \"path_to_your_image.jpg\"\n",
    "\n",
    "# Display the image\n",
    "display(Image(filename=\"Q_learning_formula.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above formula is how the action-value array is updated at each step during Q-learning\n",
    "\n",
    "- Alpha represents a learning rate (how large the update to the array should be at each time step)\n",
    "- Gamma represents a discount factor, we weight the future reward less in comparison to the immediate reward received\n",
    "- r(t+1) is the expected reward received after taking the action\n",
    "- Q(st, at) is the action-value for the current state and action\n",
    "- Q(st+1, at) is the action-value for the next state\n",
    "- Temporal difference (TD) target is the estimate of the new state based on the immediate reward (the alpha term multiplied by the positive terms in the bracket)\n",
    "- TD error is the difference TD target and the current estimate for the current state (the update term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import triangle\n",
    "Triangle_DF = pd.read_csv(\"triangle.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert data to numpy array\n",
    "Triangle_array = Triangle_DF.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[75. nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [95. 64. nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [17. 47. 82. nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [18. 35. 87. 10. nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [20.  4. 82. 47. 65. nan nan nan nan nan nan nan nan nan nan]\n",
      " [19.  1. 23. 75.  3. 34. nan nan nan nan nan nan nan nan nan]\n",
      " [88.  2. 77. 73.  7. 63. 67. nan nan nan nan nan nan nan nan]\n",
      " [99. 65.  4. 28.  6. 16. 70. 92. nan nan nan nan nan nan nan]\n",
      " [41. 41. 26. 56. 83. 40. 80. 70. 33. nan nan nan nan nan nan]\n",
      " [41. 48. 72. 33. 47. 32. 37. 16. 94. 29. nan nan nan nan nan]\n",
      " [53. 71. 44. 65. 25. 43. 91. 52. 97. 51. 14. nan nan nan nan]\n",
      " [70. 11. 33. 28. 77. 73. 17. 78. 39. 68. 17. 57. nan nan nan]\n",
      " [91. 71. 52. 38. 17. 14. 91. 43. 58. 50. 27. 29. 48. nan nan]\n",
      " [63. 66.  4. 68. 89. 53. 67. 30. 73. 16. 69. 87. 40. 31. nan]\n",
      " [ 4. 62. 98. 27. 23.  9. 70. 98. 73. 93. 38. 53. 60.  4. 23.]]\n"
     ]
    }
   ],
   "source": [
    "## print numpy array\n",
    "print(Triangle_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN values and convert each row to a list\n",
    "Triangle = [row[~np.isnan(row)].tolist() for row in Triangle_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[75.0], [95.0, 64.0], [17.0, 47.0, 82.0], [18.0, 35.0, 87.0, 10.0], [20.0, 4.0, 82.0, 47.0, 65.0], [19.0, 1.0, 23.0, 75.0, 3.0, 34.0], [88.0, 2.0, 77.0, 73.0, 7.0, 63.0, 67.0], [99.0, 65.0, 4.0, 28.0, 6.0, 16.0, 70.0, 92.0], [41.0, 41.0, 26.0, 56.0, 83.0, 40.0, 80.0, 70.0, 33.0], [41.0, 48.0, 72.0, 33.0, 47.0, 32.0, 37.0, 16.0, 94.0, 29.0], [53.0, 71.0, 44.0, 65.0, 25.0, 43.0, 91.0, 52.0, 97.0, 51.0, 14.0], [70.0, 11.0, 33.0, 28.0, 77.0, 73.0, 17.0, 78.0, 39.0, 68.0, 17.0, 57.0], [91.0, 71.0, 52.0, 38.0, 17.0, 14.0, 91.0, 43.0, 58.0, 50.0, 27.0, 29.0, 48.0], [63.0, 66.0, 4.0, 68.0, 89.0, 53.0, 67.0, 30.0, 73.0, 16.0, 69.0, 87.0, 40.0, 31.0], [4.0, 62.0, 98.0, 27.0, 23.0, 9.0, 70.0, 98.0, 73.0, 93.0, 38.0, 53.0, 60.0, 4.0, 23.0]]\n"
     ]
    }
   ],
   "source": [
    "## Triangle is now a list of lists, with each list being a row from the triangle\n",
    "print(Triangle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(Triangle[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning we need to define an environment. It's purpose being to change the agent state given an action as well as returning a reward. For my case the reward is value of the specified route. Below is my Triangle Environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriangleEnv:\n",
    "    def __init__(self, Triangle_Rewards):\n",
    "        # Triangle properties\n",
    "        self.reward_values = Triangle_Rewards ## Reward values from Triangle list of lists\n",
    "        self.rows = len(Triangle)  # Number of rows in the triangle\n",
    "        self.current_state = (0, 0)  # Starting state (i.e. top of triangle)\n",
    "        self.terminal_states = self.rows - 1  # Terminal state row index\n",
    "        self.actions = [\"Down-Left\", \"Down-Right\"]  # Possible actions\n",
    "\n",
    "        ## Initialise current state to top\n",
    "        self.current_state = (0,0)\n",
    "\n",
    "    def get_reward(self, state):\n",
    "        \"\"\"Returns the reward for a given state.\"\"\"\n",
    "\n",
    "        ## Obtain reward value for being in state\n",
    "        return self.reward_values[state[0]][state[1]]\n",
    "\n",
    "    def make_step(self, action):\n",
    "        \"\"\"Moves the agent in the specified direction and returns the new state and reward.\"\"\"\n",
    "\n",
    "        ## Assign current state\n",
    "        row, col = self.current_state\n",
    "\n",
    "        ## Move state if Down-Left\n",
    "        if action == \"Down-Left\":\n",
    "            new_state = (row + 1, col)\n",
    "        ## Move state if Down-Right\n",
    "        elif action == \"Down-Right\":\n",
    "            new_state = (row + 1, col + 1)\n",
    "\n",
    "        ## Assign new state to current state\n",
    "        self.current_state = new_state\n",
    "\n",
    "\n",
    "        ## Obtain new state reward\n",
    "        reward = self.get_reward(new_state)\n",
    "            \n",
    "        return self.current_state, reward\n",
    "\n",
    "    def check_state(self):\n",
    "        \"\"\"Check if the agent is in a terminal state.\"\"\"\n",
    "        if self.current_state[0] == self.terminal_states:\n",
    "            return \"TERMINAL\"\n",
    "        return \"NOT TERMINAL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the Q learning agent which chooses an action given a state (follows a policy) and then updates the action-value space using the Q-learning formula. The policy below utilises epsilon greedy, the policy will randomly follow an action if our randomly selected variable is below a threshold (epsilon greedy). We will randomly draw a value between 0 and 1 and if it's less than epsilon we will randomly select an action. If it is greater we choose the greediest action with respect to expected reward i.e. the argmax of Q(state, action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, num_steps = 15, num_actions = 2, alpha=0.1, gamma=0.9, epsilon=0.3):\n",
    "        ## Agent properties\n",
    "        self.num_actions = num_actions # Number of actions i.e. two\n",
    "        self.rows = num_steps # number of triangle rows\n",
    "        self.alpha = alpha # Learning rate\n",
    "        self.gamma = gamma # Future reward discount factor\n",
    "        self.epsilon = epsilon # Epsilon greedy parameter for taking random action\n",
    "        self.q_values = [] # action value array\n",
    "\n",
    "        ## Create q_value list of Q value arrays corresponding to each row\n",
    "        for idx in range(self.rows):\n",
    "            self.q_values.append(np.zeros((idx + 1, num_actions))) # state x actions\n",
    "\n",
    "        ## Random q_value initialisation\n",
    "        self.q_values = [np.random.rand(i + 1, num_actions) * 0.01 for i in range(self.rows)]  # Small random values\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose an action based on the epsilon-greedy policy.\"\"\"\n",
    "\n",
    "        ## obtain state\n",
    "        row, col = state\n",
    "        ## Choose a random action if random number is less than epsilon\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.choice(self.num_actions)  # Explore: random action\n",
    "        \n",
    "        ## Choose action based on the argmax of the current state (greedy action)\n",
    "        else:\n",
    "            return np.argmax(self.q_values[row][col])\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-value for the given state-action pair.\"\"\"\n",
    "        ## obtain current and next state\n",
    "        row, col = state\n",
    "        next_row, next_col = next_state\n",
    "\n",
    "        ## Determine the maximum Q-value for the next state (already chosen by the action)\n",
    "        max_next_q_value = np.max(self.q_values[next_row][next_col])\n",
    "\n",
    "        ## Calculate the TD target\n",
    "        td_target = reward + self.gamma * max_next_q_value\n",
    "\n",
    "        ## Calculate the TD error\n",
    "        td_error = td_target - self.q_values[row][col][action]\n",
    "\n",
    "        ## Update the Q-value for the current state-action pair\n",
    "        self.q_values[row][col][action] += self.alpha * td_error\n",
    "\n",
    "    def epsilon_change(self, epsilon_value):\n",
    "        self.epsilon = epsilon_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(environment, agent, trials=1000):\n",
    "    \"\"\"The play function runs iterations and updates Q-values.\"\"\"\n",
    "    reward_per_episode = []  # Initialize performance log\n",
    "\n",
    "    for trial in range(trials):  # Run trials\n",
    "        cumulative_reward = environment.reward_values[0][0]  # Initialize the cumulative reward for the episode\n",
    "        max_steps_per_episode = environment.rows ## max steps per episode\n",
    "        step = 0\n",
    "        Terminal = False ## for terminating experiment\n",
    "\n",
    "        # Reset the environment to the initial state for each trial\n",
    "        environment.current_state = (0, 0)\n",
    "\n",
    "        # Decrease exploration\n",
    "        if trial == np.round(0.3*trials):\n",
    "            agent.epsilon_change(0.05)\n",
    "\n",
    "        while step < max_steps_per_episode and not Terminal:  # Run until max steps or terminal state\n",
    "            old_state = environment.current_state  # Store the old state\n",
    "\n",
    "            # Choose the next action from the agent\n",
    "            action = agent.choose_action(old_state)\n",
    "    \n",
    "            # Gain reward feedback and new state from taking action in the environment\n",
    "            new_state, reward = environment.make_step(environment.actions[action])\n",
    "            \n",
    "            # Update Q-values \n",
    "            agent.learn(old_state, action, reward, new_state)\n",
    "\n",
    "            # Accumulate the reward\n",
    "            cumulative_reward += reward\n",
    "            \n",
    "            # Increment step\n",
    "            step += 1\n",
    "\n",
    "            # Check if the environment is in a terminal state\n",
    "            if environment.check_state() == \"TERMINAL\":\n",
    "                Terminal = True       \n",
    "\n",
    "        # Append the reward for the current trial to the performance log\n",
    "        reward_per_episode.append(cumulative_reward)\n",
    "\n",
    "    return reward_per_episode  # Return performance log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max reward is 1074.0, It took 719 runs to obtain maximum reward\n",
      "The max route was chosen: 5050 times\n"
     ]
    }
   ],
   "source": [
    "## Define environment\n",
    "env = TriangleEnv(Triangle) ## Pass list of lists\n",
    "\n",
    "## Define agent\n",
    "agent = QLearningAgent(env.rows)\n",
    "\n",
    "## Play game\n",
    "reward_per_trials = play(environment= env, agent = agent, trials=10000)\n",
    "\n",
    "max_reward = np.max(reward_per_trials) ## get max\n",
    "Run_to_Max = np.argmax(reward_per_trials == max_reward) ## how long did it take to get max\n",
    "\n",
    "max_count = np.sum(np.array(reward_per_trials) == max_reward)\n",
    "print(f\"The max reward is {max_reward}, It took {Run_to_Max} runs to obtain maximum reward\")\n",
    "print(f\"The max route was chosen: {max_count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum value obtained is 1074, once the agent discovers the optimal route we observe roughly half the experiments obtain the maximum reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
